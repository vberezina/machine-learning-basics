{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEJWnHzM+woTybixAlC4PC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vberezina/machine-learning-basics/blob/main/practices/LR4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ЛАБОРАТОРНАЯ РАБОТА №4. Наивный байесовский классификатор (Naive Bayes)"
      ],
      "metadata": {
        "id": "y0WD0r6OrMO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Теоретический минимум\n",
        "\n"
      ],
      "metadata": {
        "id": "4iV3-CHlrbGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Почему он «Наивный»?**\n",
        "Алгоритм делает допущение, что **все признаки независимы друг от друга** относительно целевой переменной. В реальности это почти никогда не так (например, в тексте слова «машинное» и «обучение» часто идут рядом).\n",
        "\n",
        "### **Теорема Байеса**\n",
        "\n",
        "В основе алгоритма лежит классическая теорема вероятности. Для задачи классификации она записывается так:\n",
        "\n",
        "$$P(y \\mid X) = \\frac{P(X \\mid y) \\cdot P(y)}{P(X)}$$\n",
        "\n",
        "Где:\n",
        "* $P(y \\mid X)$ — **Апостериорная вероятность:** вероятность того, что объект относится к классу $y$ при данных признаках $X$\n",
        "* $P(y)$ — **Априорная вероятность:** насколько часто этот класс встречается в выборке.\n",
        "* $P(X \\mid y)$ — **Правдоподобие:** вероятность встретить такие признаки у представителя класса $y$\n",
        "\n",
        "* $P(X)$ — **Свидетельство:** вероятность встретить такие признаки в принципе (при сравнении классов этот множитель обычно отбрасывают, так как он одинаков для всех).\n",
        "\n",
        "\n",
        "\n",
        "### **Гауссовское распределение**\n",
        "\n",
        "**Gaussian Naive Bayes** предполагает, что значения признаков распределены по нормальному (Гауссовскому) закону. Чтобы описать это распределение для каждого класса, нам нужно знать всего два параметра:\n",
        "\n",
        "1. **Среднее ($\\mu$):** центр «колокола».\n",
        "\n",
        "2. **Дисперсия ($\\sigma^2$):** ширина «колокола».\n",
        "\n",
        "\n",
        "Для классификации мы ищем класс $y$, который максимизирует вероятность:\n",
        "$$P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)$$\n",
        "\n",
        "В гауссовском варианте вероятность признака $P(x_i \\mid y)$ рассчитывается по формуле нормального распределения:\n",
        "\n",
        "$$P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma_y^2}\\right)$$\n",
        "\n",
        "### **Проблема малых чисел и Log-sum-exp**\n",
        "\n",
        "Когда признаков много, перемножение большого количества вероятностей (чисел типа $0.01$) приводит к экстремально маленьким значениям, которые компьютер округляет до нуля.\n",
        "Чтобы этого избежать, мы переходим к **логарифмам**. Вместо умножения вероятностей мы складываем их логарифмы:\n",
        "$$\\ln(P) = \\ln(P(y)) + \\sum \\ln(P(x_i \\mid y))$$\n"
      ],
      "metadata": {
        "id": "ypse9f2Pspp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Учебная задача\n"
      ],
      "metadata": {
        "id": "HNsp72LzrqSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "\n",
        "        # Инициализируем среднее, дисперсию и априорные вероятности\n",
        "        self.mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self.var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self.priors = np.zeros(n_classes, dtype=np.float64)\n",
        "\n",
        "        for idx, c in enumerate(self.classes):\n",
        "            X_c = X[y == c]\n",
        "            self.mean[idx, :] = X_c.mean(axis=0)\n",
        "            self.var[idx, :] = X_c.var(axis=0)\n",
        "            self.priors[idx] = X_c.shape[0] / float(n_samples)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = [self._predict(x) for x in X]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        posteriors = []\n",
        "\n",
        "        for idx, c in enumerate(self.classes):\n",
        "            # Используем логарифмы, чтобы избежать зануления при умножении малых чисел\n",
        "            prior = np.log(self.priors[idx])\n",
        "            posterior = np.sum(np.log(self._pdf(idx, x)))\n",
        "            posterior = posterior + prior\n",
        "            posteriors.append(posterior)\n",
        "\n",
        "        return self.classes[np.argmax(posteriors)]\n",
        "\n",
        "    def _pdf(self, class_idx, x):\n",
        "        \"\"\"Probability Density Function (Гауссовское распределение)\"\"\"\n",
        "        mean = self.mean[class_idx]\n",
        "        var = self.var[class_idx]\n",
        "        # Добавляем небольшое число (1e-9) в знаменатель, чтобы избежать деления на ноль\n",
        "        numerator = np.exp(- (x - mean)**2 / (2 * var + 1e-9))\n",
        "        denominator = np.sqrt(2 * np.pi * var + 1e-9)\n",
        "        return numerator / denominator\n",
        "\n",
        "# --- Пример использования ---\n",
        "if __name__ == \"__main__\":\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.datasets import make_classification\n",
        "\n",
        "    # Генерируем данные\n",
        "    X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=123)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "    nb = NaiveBayes()\n",
        "    nb.fit(X_train, y_train)\n",
        "    predictions = nb.predict(X_test)\n",
        "\n",
        "    accuracy = np.sum(predictions == y_test) / len(y_test)\n",
        "    print(f\"Точность модели: {accuracy * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "C275V5dmpO53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dd15b0f-58f0-47dc-9b57-d2ba95616b50"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность модели: 96.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задания\n"
      ],
      "metadata": {
        "id": "o-DMNJajsXKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание №1. Классификатор\n",
        "\n",
        "Реализуйте алгоритм Gaussian Naive Bayes, используя только библиотеку numpy. Ваш класс должен следовать стандартному интерфейсу `fit/predict`:\n",
        "\n",
        "1. В методе `fit` рассчитайте среднее значение ($\\mu$), дисперсию ($\\sigma^2$) и априорную вероятность ($P(y)$) для каждого класса.\n",
        "\n",
        "2. В методе `predict` реализуйте вычисление функции плотности вероятности (PDF) для нормального распределения.\n",
        "\n",
        "3. Используйте логарифмирование вероятностей, чтобы избежать проблемы арифметического недополнения (чисел, близких к нулю).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vk090-O_sxq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Задание №2. Обучение\n",
        "\n",
        "Проверьте работоспособность собственного классификатора на искусственно сгенерированных данных. Для этого:\n",
        "\n",
        "1. Сгенерируйте выборку из 1000 объектов и 10 признаков для бинарной классификации.\n",
        "\n",
        "2. Разделите данные на обучающую и тестовую выборки в соотношении 80/20.\n",
        "\n",
        "3. Обучите модель на тренировочных данных и выведите итоговую точность (accuracy) на тестовых данных.\n",
        "\n"
      ],
      "metadata": {
        "id": "DL9HEL0rs074"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Контрольные вопросы"
      ],
      "metadata": {
        "id": "gauZ59qMuIZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Почему классификатор называется «Наивным»? К каким последствиям для точности это может привести?\n",
        "\n",
        "2. Что такое априорная вероятность ($P(y)$) в контексте кода? Как она вычисляется в методе fit?\n",
        "\n",
        "3. Какое распределение (кроме Гауссовского) можно использовать для классификации текстовых документов?\n",
        "\n",
        "4. Как изменится работа алгоритма, если один из признаков будет сильно коррелировать с другим? Нарушит ли это работу кода технически?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FGUWTEgJuKB_"
      }
    }
  ]
}